---
title: "Multiple Regression Assumptions and Diagnostics"
subtitle: "STA6235: Modeling in Regression"
execute:
  echo: true
  warning: false
  message: false
format: 
  revealjs:
    theme: uwf2
    embed-resources: true
    slide-number: false
    width: 1600
    height: 900
    df-print: paged
    html-math-method: katex
title-slide-attributes:
    data-background-image: /Users/sseals/Library/CloudStorage/GoogleDrive-sseals@uwf.edu/My Drive/00 - Personal/R/quarto themes/title.png
    data-background-size: contain 
editor: source
pdf-separate-fragments: true
fig-align: center
---

## Introduction {background-image="/Users/sseals/Library/CloudStorage/GoogleDrive-sseals@uwf.edu/My Drive/00 - Personal/R/quarto themes/slide.png" background-size="contain"} 

- Let us now review the "checks" we will perform on our models.

    - Model assumptions
    
    - Outliers
    
    - Influence
    
    - Leverage
    
    - Multicollinearity

## Model Assumptions {background-image="/Users/sseals/Library/CloudStorage/GoogleDrive-sseals@uwf.edu/My Drive/00 - Personal/R/quarto themes/slide.png" background-size="contain"} 

- Recall the glm, $$ y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + ... \beta_k x_k + \varepsilon $$ where $\varepsilon$ is the residual error term.

- Recall that the residual error is defined as $$\varepsilon = y - \hat{y}$$ where 

    - $y$ is the observed value
    
    - $\hat{y}$ is the predicted value

- The residual tells us how far away the observation is from the response surface (the predicted value).

- Ordinary least squares estimation means that we have minimized the overall error.
    
## Model Assumptions {background-image="/Users/sseals/Library/CloudStorage/GoogleDrive-sseals@uwf.edu/My Drive/00 - Personal/R/quarto themes/slide.png" background-size="contain"} 

- Recall the regression (and ANOVA) assumptions, 

$$\varepsilon \overset{\text{iid}}{\sim} N(0, \sigma^2)$$

- **The assumptions are on the residual!**

- What this means:

    - Residuals are normally distributed
    
    - Distribution of residuals is centered at 0
    
    - Variance of residuals is some constant $\sigma^2$

## Model Assumptions {background-image="/Users/sseals/Library/CloudStorage/GoogleDrive-sseals@uwf.edu/My Drive/00 - Personal/R/quarto themes/slide.png" background-size="contain"} 

- We will assess the assumptions graphically.

    - Constant variance: scatterplot
    
    - Normal distribution: q-q plot
    
- A package was written by a former student, [`classpackage`](https://github.com/ieb2/class_package).

    - If you **are** working on the server, the package is already installed.

    - If you are **not** working on the server, you need to install the package:
    
```{r}
#| eval: false
# install.packages("devtools") - use this if R tells you it can't find install_github()
library(devtools)
install_github("ieb2/class_package", force = TRUE)
```

## Model Assumptions {background-image="/Users/sseals/Library/CloudStorage/GoogleDrive-sseals@uwf.edu/My Drive/00 - Personal/R/quarto themes/slide.png" background-size="contain"} 

- Once installed, we call the package,

```{r}
library(classpackage)
```

- While there are several functions in this package, we are interested in the `anova_check()` function.

```{r}
#| eval: false
m <- lm(y ~ x1 + x2 + ..., data = dataset)
anova_check(m)
```

- This will provide the scatterplot and the q-q plot.

## Today's Data {background-image="/Users/sseals/Library/CloudStorage/GoogleDrive-sseals@uwf.edu/My Drive/00 - Personal/R/quarto themes/slide.png" background-size="contain"}

- Recall [data from the US version of The Office](https://github.com/rfordatascience/tidytuesday/blob/master/data/2020/2020-03-17/readme.md) provided by [Tidy Tuesday](https://github.com/rfordatascience/tidytuesday/tree/master).

```{r}
library(tidyverse)
office_ratings <- read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-03-17/office_ratings.csv')
head(office_ratings, n=4)
```

## Model Assumptions {background-image="/Users/sseals/Library/CloudStorage/GoogleDrive-sseals@uwf.edu/My Drive/00 - Personal/R/quarto themes/slide.png" background-size="contain"} 

- Further, recall the model we constructed,

```{r}
m1 <- lm(imdb_rating ~ season + episode, data = office_ratings)
summary(m1)
```

## Model Assumptions {background-image="/Users/sseals/Library/CloudStorage/GoogleDrive-sseals@uwf.edu/My Drive/00 - Personal/R/quarto themes/slide.png" background-size="contain"} 

- Now, let's check the assumptions on the model,

<center>
```{r}
anova_check(m1)
```
</center>

## Model Fit: $R^2$ {background-image="/Users/sseals/Library/CloudStorage/GoogleDrive-sseals@uwf.edu/My Drive/00 - Personal/R/quarto themes/slide.png" background-size="contain"} 

- If we want to know how well the model fits the particular dataset at hand, we can look at the $R^2$.

    - $R^2$ is the proportion of variance explained by the model.
  
    - Because it is a proportion, $R^2 \in [0, 1]$ and is independent of the units of $y$.

- If $R^2 \to 0$, the model does not fit the data well; if $R^2 \to 1$, the model fits the data well.

    - Note that if $R^2=1$, all observations fall on the response surface.

$$ R^2 = \frac{\text{SSReg}}{\text{SSTot}} $$

## Model Fit: $R^2$ {background-image="/Users/sseals/Library/CloudStorage/GoogleDrive-sseals@uwf.edu/My Drive/00 - Personal/R/quarto themes/slide.png" background-size="contain"} 

- Remember that we are partitioning the variability in $y$ (SSTot), which is *constant*, into two pieces:

    - The variability explained by the regression model (SSReg).
  
    - The variability explained by outside sources (SSE).

- As predictors are added to the model, we necessarily increase SSReg / decrease SSE.

- We want a measure of model fit that is resistant to this fluctuation, $$R^2_{\text{adj}} = 1 - \left( \frac{n-1}{n-k-1} \right) \left( 1 - R^2 \right),$$ where $k$ is the number of predictor terms in the model.

## Model Fit: $R^2$ {background-image="/Users/sseals/Library/CloudStorage/GoogleDrive-sseals@uwf.edu/My Drive/00 - Personal/R/quarto themes/slide.png" background-size="contain"} 

- In our example,

```{r}
summary(m1)
```

## Model Diagnostics: Outliers {background-image="/Users/sseals/Library/CloudStorage/GoogleDrive-sseals@uwf.edu/My Drive/00 - Personal/R/quarto themes/slide.png" background-size="contain"} 

- Definition: data values that are much larger or smaller than the rest of the values in the dataset.

- We will look at the standardized residuals, $$ e_{i_{\text{standardized}}} = \frac{e_i}{\sqrt{\text{MSE}(1-h_i)}}, $$ where

    - $e_i = y_i - \hat{y}_i$ is the residual of the $i$^th^ observation
    - $h_i$ is the leverage of the $i$^th^ observation
    
- If $|e_{i_{\text{standardized}}}| > 2.5 \ \to \ $ outlier.

- If $|e_{i_{\text{standardized}}}| > 3 \ \to \ $ extreme outlier.

## Model Diagnostics:  Outliers {background-image="/Users/sseals/Library/CloudStorage/GoogleDrive-sseals@uwf.edu/My Drive/00 - Personal/R/quarto themes/slide.png" background-size="contain"} 

- We will use the `rstandard()` function to find the residuals.

- For ease of examining in large datasets, we will use it to create a "flag."

```{r, echo = TRUE, eval = FALSE}
dataset <- dataset %>%
  mutate(outlier = abs(rstandard(m))>2.5)
```

- We can count the number of outliers,

```{r, echo = TRUE, eval = FALSE}
dataset %>% count(outlier)
```

- We can just look at outliers from the dataset,

```{r, echo = TRUE, eval = FALSE}
new_dataset <- dataset %>% 
  filter(outlier == TRUE)
```

- We can also exclude outliers from the dataset,

```{r, echo = TRUE, eval = FALSE}
new_dataset <- dataset %>% 
  filter(outlier == FALSE)
```

## Model Diagnostics:  Outliers {background-image="/Users/sseals/Library/CloudStorage/GoogleDrive-sseals@uwf.edu/My Drive/00 - Personal/R/quarto themes/slide.png" background-size="contain"} 

- Let's check for outliers in our example data,

```{r}
office_ratings <- office_ratings %>% 
  mutate(outlier = abs(rstandard(m1))>2.5)
head(office_ratings, n = 3)
```

## Model Diagnostics:  Outliers {background-image="/Users/sseals/Library/CloudStorage/GoogleDrive-sseals@uwf.edu/My Drive/00 - Personal/R/quarto themes/slide.png" background-size="contain"} 

- How many data points are outliers?

```{r}
office_ratings %>% count(outlier)
```

- There are 6 outliers (as defined by the residual $\ge$ 2.5)

## Model Diagnostics:  Leverage and Influential Points {background-image="/Users/sseals/Library/CloudStorage/GoogleDrive-sseals@uwf.edu/My Drive/00 - Personal/R/quarto themes/slide.png" background-size="contain"}

- A leverage point is defined as follows:

    - A point for which $x_i$ is far from the other values. 

- An influential point is defined as follows:

    - A point that significantly affects the regression model. 

- We check these *together* using Cook's distance.

    - We will look for "spikes" in the plot.

- We use the `gg_cooksd()` function from the `lindia` package to construct the graph.

    - Note that it is built in `ggplot()` -- we can make modifications like we normally do in `ggplot()`.

```{r}
#| eval: false
library(lindia)
gg_cooksd(m, show.threshold = FALSE) + theme_bw()
```

## Model Diagnostics:  Leverage and Influential Points {background-image="/Users/sseals/Library/CloudStorage/GoogleDrive-sseals@uwf.edu/My Drive/00 - Personal/R/quarto themes/slide.png" background-size="contain"}

- Let's assess leverage and influence in the office ratings dataset.

<center>
```{r}
library(lindia)
gg_cooksd(m1, show.threshold = FALSE)
```
</center>


## Model Diagnostics:  Multicollinearity {background-image="/Users/sseals/Library/CloudStorage/GoogleDrive-sseals@uwf.edu/My Drive/00 - Personal/R/quarto themes/slide.png" background-size="contain"}

- Collinearity/multicollinearity: a correlation between two or more predictor variables affects the estimation procedure.

- We will use the variance inflation factor (VIF) to check for multicollinearity. $$ \text{VIF}_j = \frac{1}{1-R^2_j}, $$

- where

    - $j$ = the predictor of interest and $j \in \{1, 2, ..., k \}$,
    - $R^2_j$ results from regressing $x_j$ on the remaining $(k-1)$ predictors.
    
- We say that multicollinearity is present if VIF > 10.

- How do we deal with multicollinearity?

    - Easy answer: remove at least one predictor from the collinear set, then reassess VIF.
    
    - More complicated: discussing with collaborators/bosses.
    
## Model Diagnostics:  Multicollinearity {background-image="/Users/sseals/Library/CloudStorage/GoogleDrive-sseals@uwf.edu/My Drive/00 - Personal/R/quarto themes/slide.png" background-size="contain"}

- We will use the [`vif()`](https://www.rdocumentation.org/packages/car/versions/3.1-0/topics/vif) function from the `car` package.

```{r}
#| eval: false
library(car)
vif(m)
```

- Note: the `car` package overwrites some functions from `tidyverse`, so I typically do not load the full library.

- There will be a VIF value for each predictor in the model.
    
## Model Diagnostics:  Multicollinearity {background-image="/Users/sseals/Library/CloudStorage/GoogleDrive-sseals@uwf.edu/My Drive/00 - Personal/R/quarto themes/slide.png" background-size="contain"}

- Let's check the multicollinearity for our data,

```{r}
car::vif(m1)
```

- No multicollinearity is present.

## Sensitivity Analysis {background-image="/Users/sseals/Library/CloudStorage/GoogleDrive-sseals@uwf.edu/My Drive/00 - Personal/R/quarto themes/slide.png" background-size="contain"}

- We can perform sensitivity analysis to determine how much our model changes when we exclude the outliers.

    - Model 1: model using data with all observations
    - Model 2: model using data without identified outliers
  
- Questions we will ask: 

    - How different are the $\hat{\beta}_i$? 
    - Did a predictor go from being significant to non-significant? (or vice-versa?)
    - Does the direction of $\hat{\beta}_i$ change? 
    - What is the difference in $R^2$?

- We only look at sensitivity analysis **once** (i.e., only remove data points once for reanalysis).
  
    - If we keep going, we will whittle down the dataset to as close to a "perfect fit" as possible, introducing other issues.
    
## Sensitivity Analysis {background-image="/Users/sseals/Library/CloudStorage/GoogleDrive-sseals@uwf.edu/My Drive/00 - Personal/R/quarto themes/slide.png" background-size="contain"}

- Let's perform sensitivity analysis on our example model.

::: {.panel-tabset}

## which are outliers?

```{r}
office_ratings_outliers <- office_ratings %>% filter(outlier == TRUE)
head(office_ratings_outliers, n=6)
```

## full data

```{r}
m1 <- lm(imdb_rating ~ season + episode, data = office_ratings)
summary(m1)
```

## reduced data

```{r}
office_ratings_no_outliers <- office_ratings %>% filter(outlier == FALSE)
m2 <- lm(imdb_rating ~ season + episode, data = office_ratings_no_outliers)
summary(m2)
```

:::

## Wrap Up {background-image="/Users/sseals/Library/CloudStorage/GoogleDrive-sseals@uwf.edu/My Drive/00 - Personal/R/quarto themes/slide.png" background-size="contain"}

- Today we have covered

    - model assumptions
    
    - model diagnostics
    
    - sensitivity analysis
    
- Today's activity:

    - Check the model assumptions and diagnostics on the model constructed last week.
